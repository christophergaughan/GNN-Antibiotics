{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Study on Generating Novel Antibiotics Using Graph Neural Networks (GNNs)\n",
        "\n",
        "*Predicting and Designing New Antibiotics using Graph Neural Networks (GNNs)*\n",
        "\n",
        "*Christopher L. Gaughan, Ph.D.*\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The Centers for Disease Control and Prevention (CDC) has stated that the post-antibiotic era is already here. This is because bacteria have developed resistance to every antibiotic that has been developed, and common drugs like penicillin are no longer effective. The discovery of new antibiotic molecules is critical to combat the rising threat of resistant bacterial strains. This study aims to leverage **Graph Neural Networks (GNNs)** to discover and design new molecular structures with potential antibacterial properties inspired by recent breakthroughs in AI-driven drug discovery, such as the discovery of **Halicin**, an antibiotic identified using neural networks.\n",
        "\n",
        "> \"CDC's 2019 AR Threats Report includes national death and infection estimates that underscore the continued threat of AR in the United States. More than 2.8 million antimicrobial-resistant infections occur in the U.S. each year, and more than 35,000 people die as a result. When C. diff is added to these, the U.S. toll of all the threats in the report exceeds 3 million infections and 48,000 deaths.\"\n",
        "\n",
        "*2019 Antibiotic Resistance Threats Report*\n",
        "\n",
        "\n",
        "Mechanisms of Various Antibiotics\n",
        "\n",
        "![Antibiotic-Mechanisms](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5672523/bin/JOACP-33-300-g002.jpg)\n",
        "\n",
        "*J Anaesthesiol Clin Pharmacol. 2017 Jul-Sep; 33(3): 300–305.*\n",
        "\n",
        "\n",
        "## Goals\n",
        "\n",
        "Our objectives in this study are:\n",
        "1. **Develop a Predictive GNN Model**: Build and train a GNN to predict the antibacterial effectiveness of molecular structures based on their graph representations. Initially, the study will be brief in scope, focusing only on a small subset of known, active antibiotics for the training of our model.\n",
        "   \n",
        "2. **Generate Novel Antibiotic Molecules**: Use generative models like **Variational Autoencoders (VAEs)** or **Reinforcement Learning-based GNNs** to propose new molecular structures that could act as antibiotics.\n",
        "\n",
        "3. **AWS SageMaker Integration**: We will scale up the training set of known antibacterial compounds and deploy our model on AWS SageMaker to create a scalable endpoint. This will allow researchers and practitioners to input new molecular structures and receive predictions about their antibacterial activity, enabling large-scale testing.\n",
        "\n",
        "4. Optimization and Iteration: Continue refining the generated molecules using graph optimization techniques to ensure they possess drug-like properties while enhancing their antibacterial potential.\n",
        "\n",
        "## Methods\n",
        "\n",
        "### Dataset Preparation\n",
        "We begin by transforming molecular data into graph representations, with **adjacency matrices** representing atomic bonds and **node features** representing atomic properties (e.g., atomic number, charge, molecular weight). The dataset consists of known antibiotic compounds, which will be used to train our GNN model.\n",
        "\n",
        "### GNN Architecture\n",
        "The Graph Neural Network (GNN) encodes molecular graphs into vector representations by passing information (or \"messages\") between atoms and bonds. These graph embeddings are then used to predict whether a molecule is antibacterial or non-antibacterial, providing the foundation for further molecular generation.\n",
        "\n",
        "### AWS SageMaker Scaling\n",
        "After developing a robust model, the next step is to deploy it on **AWS SageMaker**, where we will create an endpoint that accepts molecular inputs (SMILES strings, for example). This endpoint will allow users to test new molecular structures at scale by receiving predictions for their antibacterial activity. The cloud infrastructure will enable rapid scaling and parallel processing, making the solution suitable for real-world applications.\n",
        "\n",
        "### Evaluation and Optimization\n",
        "The model will be evaluated using metrics such as **ROC-AUC**, **Accuracy**, and **Precision-Recall**. After identifying promising molecular structures, they will undergo iterative optimization using graph-based techniques to refine their drug-likeness, synthetic feasibility, and antibacterial potential further.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This study serves as the foundation for scaling the discovery of antibiotics through machine learning. By leveraging **Graph Neural Networks (GNNs)** and **AWS SageMaker**, we aim to create a scalable and efficient platform for discovering novel antibiotics. In the future, this system will provide an endpoint through which new molecules can be entered and tested for antibacterial efficacy, contributing to the ongoing fight against antibiotic resistance."
      ],
      "metadata": {
        "id": "iVc8V0cAcuZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary for Antibiotic Compounds Dataset\n",
        "\n",
        "| **Field**            | **Description**                                                                                                                                   |\n",
        "|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **cid**              | The compound identifier (CID), a unique number assigned to the compound in the PubChem database.                                                  |\n",
        "| **cmpdname**         | The name of the compound.                                                                                                                         |\n",
        "| **cmpdsynonym**      | Synonyms for the compound. This can include alternate names, trade names, or common names.                                                        |\n",
        "| **mw**               | The molecular weight of the compound (in grams per mole).                                                                                         |\n",
        "| **mf**               | The molecular formula of the compound, representing the number and types of atoms.                                                                |\n",
        "| **polararea**        | The polar surface area of the compound.                                                                                                           |\n",
        "| **xlogp**            | A measure of the hydrophobicity (logarithm of the partition coefficient between octanol and water) of the compound.                                |\n",
        "| **heavycnt**         | The number of heavy atoms in the compound (atoms that are not hydrogen).                                                                          |\n",
        "| **hbonddonor**       | The number of hydrogen bond donors in the compound, which are atoms that can donate hydrogen atoms to form hydrogen bonds.                        |\n",
        "| **hbondacc**         | The number of hydrogen bond acceptors in the compound, which are atoms that can accept hydrogen atoms to form hydrogen bonds.                     |\n",
        "| **rotatablebond**    | The number of rotatable bonds in the compound, which indicates the flexibility of the molecule.                                                   |\n",
        "| **IUPAC Name**       | The International Union of Pure and Applied Chemistry (IUPAC) name of the compound, following standardized chemical nomenclature.                 |\n",
        "| **Isomeric SMILES**  | The Simplified Molecular Input Line Entry System (SMILES) string, representing the structure of the compound, including stereochemistry information. |\n",
        "| **InChIKey**         | A hashed version of the InChI string, providing a unique identifier for the compound.                                                             |\n",
        "| **InChI**            | The International Chemical Identifier (InChI), a textual representation of the compound’s structure.                                              |\n",
        "| **annotation**       | Additional annotation about the compound, including therapeutic uses and classifications, such as `Anti-Infective Agent`, `Antiprotozoal Agent`, or `Anti-Bacterial Agents`. |\n"
      ],
      "metadata": {
        "id": "Vp0J2qdCEFDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB6kytijC4fF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if the file exists\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/PubChem_compound_text_Antibiotics.csv'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists and ready to load!\")\n",
        "else:\n",
        "    print(\"File not found. Check the file path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "6whTlDziFtyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Get basic statistics about the dataset\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Display data types to ensure consistency\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check the first few rows to ensure everything is loaded correctly\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "VZyBlUoBF3Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing xlogp Values Using Predictive Modeling\n",
        "\n",
        "#### What:\n",
        "We have found that 17 values in the xlogp column are missing, and the variability in the existing values is quite high. Simple imputation (e.g., mean or median) could introduce bias, as the distribution of xlogp is broad and not concentrated around a single value. Therefore, we will use predictive modeling to estimate these missing values based on other available features in the dataset.\n",
        "\n",
        "#### Why:\n",
        "Predictive imputation uses the relationships between the missing values and other features to provide more accurate estimations. In this case, we can build a model to predict xlogp using the rest of the columns as inputs. This approach is more robust than simply filling missing values with a constant, as it takes into account the actual distribution and relationships in the data.\n",
        "\n",
        "#### How:\n",
        "1. We will first prepare the dataset, ensuring that no columns used for prediction have missing values themselves.\n",
        "2. We'll split the dataset into two parts:\n",
        "    - One with known xlogp values (to train the model).\n",
        "    - One with missing xlogp values (to impute).\n",
        "3. A regression model (e.g., RandomForestRegressor) will be trained on the known values to predict xlogp.\n",
        "4. We will then use this model to predict the missing values and fill them back into the dataset.\n"
      ],
      "metadata": {
        "id": "0sjn4N_fJJP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/PubChem_compound_text_Antibiotics.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where xlogp is missing for training, and separate the rows with missing xlogp for prediction\n",
        "data_with_xlogp = data.dropna(subset=['xlogp'])\n",
        "data_without_xlogp = data[data['xlogp'].isnull()]\n",
        "\n",
        "# Selecting features to use for predicting xlogp (excluding 'xlogp' itself)\n",
        "features = ['mw', 'polararea', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'exactmass', 'monoisotopicmass']\n",
        "\n",
        "# Training data\n",
        "X_train = data_with_xlogp[features]\n",
        "y_train = data_with_xlogp['xlogp']\n",
        "\n",
        "# The rows without xlogp, which need to be predicted\n",
        "X_pred = data_without_xlogp[features]\n",
        "\n",
        "# Train a Random Forest Regressor model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the missing xlogp values\n",
        "predicted_xlogp = rf.predict(X_pred)\n",
        "\n",
        "# Impute the missing xlogp values with the predicted values\n",
        "data.loc[data['xlogp'].isnull(), 'xlogp'] = predicted_xlogp\n",
        "\n",
        "# Check the imputation\n",
        "print(data[['xlogp']].isnull().sum())  # This should print 0, meaning all missing values have been filled.\n"
      ],
      "metadata": {
        "id": "-Jsk4bN4JviO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Extracting the features for rows where xlogp is missing\n",
        "X_missing = data_without_xlogp.drop(['xlogp'], axis=1)\n",
        "\n",
        "# Ensure that columns in X_train and X_missing are aligned\n",
        "X_missing = X_missing[X_train.columns]\n",
        "\n",
        "# Predict the xlogp values for the missing data\n",
        "predicted_xlogp_missing = rf.predict(X_missing)\n",
        "\n",
        "# Now plot the original vs predicted xlogp values and highlight the imputed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the actual vs predicted xlogp values for training data (in blue)\n",
        "sns.scatterplot(x=y_train, y=rf.predict(X_train), label=\"Predicted (Train)\", color=\"blue\")\n",
        "\n",
        "# Plot the red circles for imputed xlogp values\n",
        "sns.scatterplot(x=data_without_xlogp['xlogp'], y=predicted_xlogp_missing,\n",
        "                label=\"Imputed xlogp (Red Circles)\", color=\"red\", marker=\"o\", s=100)\n",
        "\n",
        "# Plot the perfect prediction line\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
        "         color='green', linestyle='--')\n",
        "\n",
        "plt.title('Actual vs Predicted xlogp Values (with Imputed Highlighted)')\n",
        "plt.xlabel('Actual xlogp')\n",
        "plt.ylabel('Predicted xlogp')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ip7X76wULlTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the features for rows where xlogp is missing\n",
        "X_missing = data_without_xlogp.drop(['xlogp'], axis=1)\n",
        "\n",
        "# Ensure that columns in X_train and X_missing are aligned\n",
        "X_missing = X_missing[X_train.columns]\n",
        "\n",
        "# Predict the xlogp values for the missing data\n",
        "predicted_xlogp_missing = rf.predict(X_missing)\n",
        "\n",
        "# Now plot the actual vs predicted xlogp values and highlight the imputed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the actual vs predicted xlogp values for training data (in blue)\n",
        "sns.scatterplot(x=y_train, y=rf.predict(X_train), label=\"Predicted (Train)\", color=\"blue\")\n",
        "\n",
        "# Plot the red circles for imputed xlogp values\n",
        "sns.scatterplot(x=predicted_xlogp_missing, y=predicted_xlogp_missing,\n",
        "                label=\"Imputed xlogp (Red Circles)\", color=\"red\", marker=\"o\", s=100)\n",
        "\n",
        "# Plot the perfect prediction line\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
        "         color='green', linestyle='--')\n",
        "\n",
        "plt.title('Actual vs Predicted xlogp Values (with Imputed Highlighted)')\n",
        "plt.xlabel('Actual xlogp')\n",
        "plt.ylabel('Predicted xlogp')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3MD7Fo1uMBhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for correlation matrix\n",
        "numeric_data_with_xlogp = data_with_xlogp.select_dtypes(include=[float, int])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_data_with_xlogp.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "heatmap.set_title('Correlation Matrix of Key Variables', fontdict={'fontsize': 12}, pad=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9f50FB3PNtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) and Correlation Simplification\n",
        "\n",
        "## Why Highly Correlated Variables Provide Redundant Information\n",
        "\n",
        "In any dataset, when two or more variables are highly correlated, it means that they provide similar information. Correlation measures the linear relationship between variables, and a correlation coefficient close to +1 or -1 indicates that one variable can be approximately predicted from the other. In simpler terms, if two variables are highly correlated (either positively or negatively), they are essentially measuring the same aspect of the data.\n",
        "\n",
        "### Positive Correlation (+1)\n",
        "A correlation of +1 indicates a perfect positive relationship, where as one variable increases, the other also increases proportionally. This means the two variables are redundant because they move together in the same direction.\n",
        "\n",
        "### Negative Correlation (-1)\n",
        "A correlation of -1 indicates a perfect negative relationship, where as one variable increases, the other decreases proportionally. In this case, the variables are inversely related, but still provide overlapping information.\n",
        "\n",
        "### Why Redundant Variables Are a Problem\n",
        "When variables are highly correlated, they introduce **multicollinearity** into statistical models like regression. Multicollinearity can cause problems because:\n",
        "- **Unstable Coefficients**: The model may struggle to determine which variable is more important, leading to large standard errors in estimated coefficients.\n",
        "- **Overfitting**: Including too many highly correlated variables increases the risk of overfitting the model to the training data, which may reduce its ability to generalize to new data.\n",
        "\n",
        "## Purpose of Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction technique that helps us simplify a dataset with many features (variables) into fewer dimensions, while preserving the essential information. By transforming correlated variables into a new set of uncorrelated variables called **principal components**, PCA allows us to:\n",
        "1. **Reduce Redundancy**: By combining correlated variables into fewer components, we remove redundant information.\n",
        "2. **Improve Model Performance**: Simplifying the dataset can help reduce the risk of overfitting and improve the stability of models.\n",
        "3. **Visualize Data**: PCA also allows us to visualize high-dimensional data in a lower-dimensional space, helping to understand its structure.\n",
        "\n",
        "## Why We Are Performing This Analysis\n",
        "\n",
        "In our dataset, we observed that some variables are highly correlated with each other, indicating redundancy. To simplify the analysis, reduce multicollinearity, and create a more efficient model, we are applying PCA. Specifically, we aim to:\n",
        "- **Reduce the number of features**: By using PCA, we can combine the information from correlated features into fewer principal components without losing critical information.\n",
        "- **Identify key patterns**: PCA helps us identify the underlying structure of the data and the relationships between variables.\n",
        "- **Prepare data for modeling**: After applying PCA, we will use the principal components as features in our predictive models, improving their performance by removing redundant information.\n",
        "\n",
        "## Steps for Applying PCA\n",
        "\n",
        "1. **Standardize the Data**: Before performing PCA, we need to standardize the data so that each feature has a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the variance in data.\n",
        "2. **Fit PCA**: We will fit PCA to the standardized data and compute the principal components.\n",
        "3. **Visualize Variance Explained**: PCA will return a set of principal components. We will examine how much variance is explained by each component, helping us decide how many components to keep.\n",
        "4. **Transform Data**: Finally, we will use the principal components to transform the dataset, replacing the original variables with the new components.\n"
      ],
      "metadata": {
        "id": "FbYN6RrtPqlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a correlation threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Find pairs of variables with high correlation\n",
        "high_corr_pairs = correlation_matrix.unstack().sort_values(kind=\"quicksort\").drop_duplicates()\n",
        "\n",
        "# Filter out only pairs that exceed the threshold\n",
        "high_corr_pairs = high_corr_pairs[(high_corr_pairs > threshold) | (high_corr_pairs < -threshold)]\n",
        "\n",
        "# Display the high correlation pairs\n",
        "print(high_corr_pairs)\n"
      ],
      "metadata": {
        "id": "UuMx_W2oN2hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Observations:\n",
        "1. Mass-related Variables (mw, exactmass, monoisotopicmass): These variables are perfectly correlated with one another (correlation of 1.0). In any subsequent analysis, we can keep just one of these and discard the others to reduce redundancy.\n",
        "\n",
        "2. Polar Surface Area (polararea): This variable is highly correlated with several other features, including complexity (0.842), hbondacc (0.911), and mw (0.880). We can consider keeping only one of these highly correlated variables.\n",
        "\n",
        "3. Hydrogen Bond Acceptors (hbondacc): This feature is strongly correlated with many other variables, including heavy atom count (0.899), molecular weight (0.913), and complexity (0.873). This suggests that hbondacc overlaps with a lot of the other variables, and we can consider dropping some.\n",
        "\n",
        "4. Complexity: Complexity is highly correlated with molecular weight (0.983) and heavy atom count (0.986). Like the mass variables, complexity provides redundant information in combination with these features."
      ],
      "metadata": {
        "id": "wCzYCtnUQQzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Perform PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique. It is especially useful in cases where the dataset contains highly correlated features. Highly correlated variables (positive or negative) provide redundant information. PCA transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first principal component (PC1), the second greatest variance on the second component (PC2), and so on.\n",
        "\n",
        "This allows us to simplify the dataset while retaining most of its variability. By focusing on a reduced number of principal components, we can capture the essential patterns of the data without having to process all of the original features.\n",
        "\n",
        "In this case, many of our features are highly correlated, and PCA will help us identify the most important features contributing to the variability in our data.\n"
      ],
      "metadata": {
        "id": "VWysyMjDQ0kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Select the features for PCA (excluding 'cid', 'cmpdname', etc.)\n",
        "selected_features = [\n",
        "    'mw', 'polararea', 'complexity', 'heavycnt', 'hbonddonor', 'hbondacc',\n",
        "    'rotbonds', 'exactmass', 'monoisotopicmass', 'charge',\n",
        "    'covalentunitcnt', 'totalatomstereocnt', 'definedatomstereocnt',\n",
        "    'undefinedatomstereocnt', 'totalbondstereocnt', 'definedbondstereocnt',\n",
        "    'undefinedbondstereocnt', 'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'annothitcnt'\n",
        "]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_with_xlogp[selected_features])\n",
        "\n",
        "# Run PCA\n",
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Display explained variance per component\n",
        "for i, variance in enumerate(explained_variance, 1):\n",
        "    print(f'PC{i}: {variance * 100:.2f}%')\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--', color='b')\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance (%)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Create a DataFrame to explore the PCA components\n",
        "pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
        "pca_df.head()\n"
      ],
      "metadata": {
        "id": "2G6AwD7_QAHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Interpret PCA Results\n",
        "\n",
        "When performing PCA, the output provides important insights into the structure of the dataset. Here’s how to interpret the results step by step:\n",
        "\n",
        "#### 1. **Explained Variance Ratio**\n",
        "   The **explained variance ratio** tells us how much of the total variance in the data is captured by each principal component (PC). Each PC is an orthogonal (uncorrelated) linear combination of the original variables, and the total variance in the dataset is distributed across these PCs.\n",
        "\n",
        "   - **PC1** will explain the largest amount of variance in the data.\n",
        "   - **PC2** will explain the second largest, and so on.\n",
        "\n",
        "   For example, if PC1 has an explained variance ratio of **40%**, that means 40% of the variance in the original data is captured by this component alone.\n",
        "\n",
        "   **Interpreting the first few PCs:**\n",
        "   - The first few principal components (usually PC1, PC2, PC3, etc.) are the most important because they capture most of the variability in the data. Components with very low explained variance (e.g., PC10, PC11) are less important and can be ignored if we want to reduce the dimensionality.\n",
        "   \n",
        "   You can use a rule of thumb that **selects the number of components** that explain around **85% to 95%** of the variance to retain most of the information.\n",
        "\n",
        "#### 2. **Cumulative Explained Variance Plot**\n",
        "   This plot shows how much variance is cumulatively explained by increasing the number of principal components. Typically, the curve will show that most of the variance is captured by the first few components, and then it levels off.\n",
        "\n",
        "   - If the curve flattens after a certain number of PCs (e.g., after PC4), it indicates that **adding more components beyond that point doesn't significantly increase the explained variance.**\n",
        "   - For example, if the plot shows that **90% of the variance** is explained by the first **4 components**, you may choose to keep those 4 components and discard the rest, simplifying the dataset without losing too much information.\n",
        "\n",
        "#### 3. **Principal Component Scores**\n",
        "   The transformed dataset after applying PCA gives us the **principal component scores** for each observation. These scores represent the coordinates of each observation in the new space defined by the principal components. They can be used for further analysis or as input to machine learning models.\n",
        "\n",
        "   - **PC1, PC2, etc.** form new axes where each observation is represented as a point in this new space. These axes capture the most significant patterns in the data.\n",
        "   - **Higher variance along a component** means that component contains more useful information about how the observations vary.\n",
        "\n",
        "#### 4. **Loadings (Contribution of Original Features to Each PC)**\n",
        "   Each principal component is a linear combination of the original features. The coefficients (also called **loadings**) for each original feature in these combinations tell us how much each feature contributes to each principal component.\n",
        "\n",
        "   - **Positive or negative loadings** show the direction of the relationship with that component. Large positive or negative values indicate a strong influence on the component.\n",
        "   - Features with high loadings on the first few principal components are the most important in explaining the variance in the dataset.\n",
        "\n",
        "   **Example Interpretation:**\n",
        "   - If PC1 has high positive loadings for features like `mw` (molecular weight) and `hbonddonor` (hydrogen bond donors), it means that these features are strongly associated with the first principal component.\n",
        "\n",
        "#### 5. **Dimensionality Reduction**\n",
        "   After running PCA, you can choose to keep only the principal components that explain a high percentage of variance (e.g., the first 4 or 5). This reduced set of components can then be used for further analysis or modeling.\n",
        "\n",
        "   - By reducing the dimensionality, we can simplify the data and reduce computational complexity, while still retaining the majority of the original information.\n",
        "\n",
        "### Practical Steps\n",
        "\n",
        "1. **Check how many components explain ~85-95% of the variance.** This will help decide how many PCs to retain.\n",
        "2. **Inspect the loadings** for the first few principal components to understand which features are contributing the most.\n",
        "3. **Use the reduced dataset** (with the selected number of principal components) for further analysis or as input to machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "Ek1TS2LvRSeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explained variance from the PCA\n",
        "pca_explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot the explained variance for the top PCA components\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.bar(range(1, len(pca_explained_variance)+1), pca_explained_variance, color='blue', alpha=0.7)\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xticks(range(1, len(pca_explained_variance)+1))\n",
        "plt.grid(True)\n",
        "\n",
        "# Annotate with variance percentages\n",
        "for i, v in enumerate(pca_explained_variance):\n",
        "    plt.text(i + 1, v + 0.01, f\"{v*100:.2f}%\", ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TaUE8eMoQ6Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns\n",
        "numerical_columns = data_with_xlogp.select_dtypes(include=[np.number])\n",
        "\n",
        "# Variance of original features (only for numerical columns)\n",
        "original_variance = numerical_columns.var()\n",
        "\n",
        "# Plot comparison of variance in original features and PCA components\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot variance of original features\n",
        "plt.subplot(1, 2, 1)\n",
        "original_variance.plot(kind='bar', color='green', alpha=0.7)\n",
        "plt.title('Variance of Original Features (Numerical)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Variance')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot explained variance of PCA components\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(1, len(pca_explained_variance)+1), pca_explained_variance, color='blue', alpha=0.7)\n",
        "plt.title('Explained Variance of PCA Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xticks(range(1, len(pca_explained_variance)+1))\n",
        "plt.grid(True)\n",
        "\n",
        "# Annotate with variance percentages for PCA\n",
        "for i, v in enumerate(pca_explained_variance):\n",
        "    plt.text(i + 1, v + 0.01, f\"{v*100:.2f}%\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LE_2jT1pSvzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing PCA Results with Original Numerical Features\n",
        "\n",
        "To ensure an accurate comparison between PCA components and the original dataset, we are focusing on **numerical** features only. The `cmpdname`, `cmpdsynonym`, and other non-numerical columns have been excluded from this analysis.\n",
        "\n",
        "- **Original Features**: The variance for each numerical feature is plotted to show how much variability each feature captures within the dataset.\n",
        "- **PCA Components**: The explained variance ratio for each principal component is plotted to demonstrate how well the PCA components summarize the data.\n",
        "\n",
        "This comparison helps us understand how much information is retained after reducing the dimensionality of the dataset with PCA.\n"
      ],
      "metadata": {
        "id": "XAG9uUTfTLFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of components to keep (for example, keeping 10 components)\n",
        "num_components_to_keep = 10  # Adjust this based on your PCA analysis\n",
        "\n",
        "# Ensure numerical_columns is defined if not already done\n",
        "numerical_columns = data_with_xlogp.select_dtypes(include=[np.number])  # Select only numerical columns\n",
        "\n",
        "# Display the principal components without using ace_tools\n",
        "pca_components = pd.DataFrame(pca.components_[:num_components_to_keep],\n",
        "                              columns=numerical_columns.columns[:21],  # Ensure correct number of columns\n",
        "                              index=[f'PC{i+1}' for i in range(num_components_to_keep)])\n",
        "\n",
        "# Display the principal components DataFrame\n",
        "print(\"Principal Components after PCA:\")\n",
        "pca_components\n",
        "\n"
      ],
      "metadata": {
        "id": "uoRSGCoNT2in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components Analysis (PCA) - Results\n",
        "\n",
        "After running PCA on the dataset, we retain **8 components** that together explain 95% of the variance in the data. Each of these components is a linear combination of the original features, and the amount each feature contributes to each component is given by the loadings.\n",
        "\n",
        "- **Variance Explained**: The first few components capture the majority of the variance, allowing us to reduce the dimensionality of the data without losing significant information.\n",
        "- **Loadings**: These indicate how much each original feature contributes to a principal component. High absolute values indicate stronger contributions from that feature.\n"
      ],
      "metadata": {
        "id": "c7Ye2QkeUNy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the number of principal components to keep\n",
        "num_components_to_keep = 8\n",
        "\n",
        "# Update numerical_columns to reflect only those columns used in PCA\n",
        "used_columns = numerical_columns.columns[:len(pca.components_[0])]\n",
        "\n",
        "# Extract the top principal components\n",
        "pca_components_to_keep = pd.DataFrame(pca.components_[:num_components_to_keep],\n",
        "                                      columns=used_columns,\n",
        "                                      index=[f'PC{i+1}' for i in range(num_components_to_keep)])\n",
        "\n",
        "# Only display the top contributing variables for each principal component\n",
        "# We can display the absolute highest loadings for each component\n",
        "top_contributing_features = pca_components_to_keep.apply(lambda x: x.nlargest(5).index, axis=1)\n",
        "\n",
        "# Print the components that we will keep\n",
        "print(\"Top contributing features for each principal component:\")\n",
        "print(top_contributing_features)\n"
      ],
      "metadata": {
        "id": "gPB32ErQVHZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The first two principal components (PC1 and PC2) contribute significantly to explaining the variance, and they have 10 key features combined. Here’s a more refined breakdown:\n",
        "\n",
        "**PC1 captures variability primarily from features such as:**\n",
        "\n",
        "* rotbonds\n",
        "* hbondacc\n",
        "* cid\n",
        "* complexity\n",
        "* polararea\n",
        "\n",
        "**PC2 captures variability from features such as:**\n",
        "\n",
        "* definedbondstereocnt\n",
        "* undefinedbondstereocnt\n",
        "* polararea\n",
        "* gpidcnt\n",
        "* gpfamilycnt\n",
        "\n",
        "The 10 features in PC1 and PC2 give us a broad understanding of which features are influencing the first two principal components the most, which in turn explain a large portion of the variance in the dataset."
      ],
      "metadata": {
        "id": "ER3ak_nkW4c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Apply PCA to the selected numerical columns\n",
        "numerical_columns_for_pca = numerical_columns[['mw', 'polararea', 'complexity', 'xlogp', 'heavycnt',\n",
        "                                               'hbonddonor', 'hbondacc', 'rotbonds', 'exactmass',\n",
        "                                               'monoisotopicmass', 'charge', 'covalentunitcnt',\n",
        "                                               'isotopeatomcnt', 'totalatomstereocnt', 'definedatomstereocnt',\n",
        "                                               'undefinedatomstereocnt', 'totalbondstereocnt',\n",
        "                                               'definedbondstereocnt', 'undefinedbondstereocnt', 'pclidcnt',\n",
        "                                               'gpidcnt']].values  # Remove the feature names here by using .values\n",
        "\n",
        "# Apply PCA to the data\n",
        "pca_components = pca.transform(numerical_columns_for_pca)\n",
        "\n",
        "# Define the target variable (xlogp values)\n",
        "target = data_with_xlogp['xlogp'].values\n",
        "\n",
        "# Check the shape to ensure consistency\n",
        "print(f\"PCA Components shape: {pca_components.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_components, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the Random Forest Regressor to the training data\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print out the model performance\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "Zr3cHLOFaukh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Performance Analysis\n",
        "\n",
        "We applied **Principal Component Analysis (PCA)** to reduce the dimensionality of the dataset, followed by training a **Random Forest Regressor** model to predict the partition coefficient (*xlogp*) of the compounds. Below, we discuss the key results and future steps:\n",
        "\n",
        "### Results:\n",
        "\n",
        "- **PCA Components Shape**: (326, 21)  \n",
        "    - After applying PCA, we retained 21 components from the original dataset.\n",
        "- **Target Shape**: (326,)  \n",
        "    - The target variable (xlogp) has 326 samples, matching the size of the PCA components.\n",
        "- **Mean Squared Error (MSE)**: 7.99  \n",
        "    - The MSE represents the average squared difference between the actual and predicted *xlogp* values. A higher value indicates more significant deviations between predictions and ground truth.\n",
        "- **R² Score**: 0.58  \n",
        "    - The R² score measures how much of the variance in the target variable is explained by the model. A score of **0.58** means that approximately **58%** of the variance in *xlogp* is captured by the model.\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "The **R² Score** of 0.58 indicates that the model performs moderately well but leaves room for improvement. Although **58%** of the variation in *xlogp* is explained by the 21 principal components, the **Mean Squared Error** suggests there is still some degree of error in the predictions.\n",
        "\n",
        "### Suggestions for Improvement:\n",
        "\n",
        "1. **Hyperparameter Tuning**:  \n",
        "    The current model uses default settings for the Random Forest Regressor. Hyperparameter tuning (e.g., adjusting the number of trees, maximum depth, and minimum samples per split) may improve model accuracy.\n",
        "   \n",
        "2. **Feature Importance & Selection**:  \n",
        "    Analyzing the **importance** of the PCA components can help identify the most informative components. Reducing the less important ones might streamline the model without sacrificing accuracy.\n",
        "\n",
        "3. **Alternative Models**:  \n",
        "    Testing with other models, such as **Gradient Boosting**, **XGBoost**, or even **neural networks**, may offer better predictive performance.\n",
        "\n",
        "4. **Expanding Dataset**:  \n",
        "    Increasing the size of the dataset could help the model generalize better, leading to improved predictions on unseen data.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The current model, while functional, leaves room for improvement. By tuning hyperparameters, selecting important features, or exploring alternative models, we can likely improve the model's predictive performance. Additionally, expanding the dataset may further enhance accuracy.\n"
      ],
      "metadata": {
        "id": "bfvLgOrf4v2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Did We Use More Features Than Those Identified by PCA?\n",
        "\n",
        "When we performed **Principal Component Analysis (PCA)**, we aimed to reduce the dimensionality of the dataset while retaining as much variance as possible. PCA identified **21 principal components**, each being a combination of the original features. These components captured the majority of the variance in the dataset, with **80% of the variance explained** by the first 10 components.\n",
        "\n",
        "However, we continued to use **more features** (from the original dataset) in the model for several reasons:\n",
        "\n",
        "### 1. **PCA Transforms Features, Not Eliminates Them**\n",
        "PCA doesn’t necessarily eliminate features but **transforms** them into linear combinations of the original variables. While we reduced the dataset to 21 components, these components are derived from the original feature set. Therefore, even though we retained 21 principal components, they still encompass information from **all original variables**.\n",
        "\n",
        "### 2. **PCA Does Not Always Select 'Physical' Features**\n",
        "PCA creates new variables (principal components) that are combinations of the original features. These new variables may not directly correspond to individual physical features in the dataset, but they contain combinations of the original data that maximize variance. Thus, **using only the features identified by PCA** would omit important information that is embedded in the original features.\n",
        "\n",
        "### 3. **Model Accuracy and Feature Engineering**\n",
        "While PCA is useful for reducing dimensionality, we opted to use the broader feature set to **enhance model accuracy**. By using the original features along with PCA-transformed components, we ensure that no important information is lost. This is particularly important when training machine learning models, where leaving out certain features prematurely may lead to reduced accuracy.\n",
        "\n",
        "### 4. **Exploring All Features for Future Models**\n",
        "We used the larger set of features because our goal is to eventually improve model accuracy and identify key patterns in the data. By keeping a larger feature set, we leave room for exploring different machine learning models, hyperparameters, and even combinations of the original features with PCA components. This allows for more flexibility in **feature selection** and testing which set of features yields the best model performance.\n",
        "\n",
        "### Conclusion:\n",
        "PCA helps us simplify and reduce data complexity, but it doesn’t always mean that fewer features are required. We kept the broader feature set to ensure that our model captures the complete range of patterns in the data, while still benefiting from the dimensionality reduction that PCA offers.\n"
      ],
      "metadata": {
        "id": "2lBaeW2F5F1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's tune Hyperparameters to see if we can get a better result from our RandomForest Regressor"
      ],
      "metadata": {
        "id": "sKDtJbfo5-8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the hyperparameter grid to search over\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 300, 500, 1000],  # Number of trees\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of each tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at a leaf node\n",
        "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Setup the RandomizedSearchCV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_regressor,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,  # Number of different combinations to try\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    verbose=2,  # Show more information while training\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    random_state=42  # For reproducibility\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters: \", random_search.best_params_)\n",
        "\n",
        "# Evaluate the tuned model on the test set\n",
        "y_pred_tuned = random_search.best_estimator_.predict(X_test)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_tuned)\n",
        "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
        "\n",
        "# Print the performance of the tuned model\n",
        "print(f\"Tuned Mean Squared Error: {mse_tuned}\")\n",
        "print(f\"Tuned R^2 Score: {r2_tuned}\")\n"
      ],
      "metadata": {
        "id": "Fue6_Z2L33JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning Methodology\n",
        "\n",
        "In this step, we optimized the performance of our **Random Forest Regressor** model by tuning its hyperparameters using **RandomizedSearchCV**. Hyperparameter tuning is essential because default model settings may not always provide the best possible performance. RandomizedSearchCV automates the process of exploring different combinations of hyperparameters to identify the best configuration.\n",
        "\n",
        "### Why Hyperparameter Tuning?\n",
        "\n",
        "Random forests have several key hyperparameters that influence model performance, including:\n",
        "- **n_estimators**: The number of decision trees in the forest.\n",
        "- **max_depth**: The maximum depth of each decision tree.\n",
        "- **min_samples_split**: The minimum number of samples required to split an internal node.\n",
        "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
        "- **bootstrap**: Whether to use bootstrap samples when building trees.\n",
        "\n",
        "Tuning these hyperparameters allows us to control the model's complexity, avoid overfitting, and optimize prediction accuracy.\n",
        "\n",
        "### Method\n",
        "\n",
        "We used **RandomizedSearchCV** to perform hyperparameter tuning:\n",
        "1. **RandomizedSearchCV** randomly selects combinations of hyperparameters from a predefined search space. This is more efficient than an exhaustive search and reduces computational cost.\n",
        "2. We specified the following search space for the random forest:\n",
        "   - `n_estimators`: Number of trees (between 100 and 500).\n",
        "   - `max_depth`: Maximum depth of trees (None or between 10 and 50).\n",
        "   - `min_samples_split`: Minimum number of samples to split a node (between 2 and 10).\n",
        "   - `min_samples_leaf`: Minimum number of samples required at a leaf node (between 1 and 4).\n",
        "   - `bootstrap`: Whether bootstrap sampling is used (True or False).\n",
        "3. **RandomizedSearchCV** was set to perform 5-fold cross-validation, where the data is split into 5 subsets. The model is trained on 4 of the subsets and validated on the remaining one. This process is repeated 5 times with different subsets, ensuring robustness in the evaluation.\n",
        "4. The best combination of hyperparameters is selected based on the model's performance on the validation set.\n",
        "\n",
        "### Results\n",
        "\n",
        "The best hyperparameters identified by **RandomizedSearchCV** were:\n",
        "- `n_estimators`: 300\n",
        "- `min_samples_split`: 2\n",
        "- `min_samples_leaf`: 1\n",
        "- `max_depth`: None\n",
        "- `bootstrap`: True\n",
        "\n",
        "The performance of the tuned model was:\n",
        "- **Mean Squared Error (MSE)**: 8.23\n",
        "- **R² Score**: 0.568\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The hyperparameter tuning did not significantly improve the model compared to the default settings. This may suggest that the original settings were already close to optimal. Despite this, hyperparameter tuning is a crucial step in ensuring that we extract the best possible performance from our models. Future steps could involve experimenting with different models or using more advanced techniques such as **GridSearchCV** or ensemble methods.\n",
        "\n",
        "This is not an unexpected result. Tuning of hyperparamters doesn't always improve the performance of the moderl. However, it is always a 'first stop' along the way because the `RandomizedSearchCV` module can do this quickly and is convenient to use."
      ],
      "metadata": {
        "id": "tvgGxrM57eWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance & Selection:\n",
        "\n",
        "Analyzing the importance of the PCA components can help identify the most informative components. Reducing the less important ones might streamline the model without sacrificing accuracy."
      ],
      "metadata": {
        "id": "afraYl_-8Z3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " So let's go ahead and analyze the feature importance of the PCA components to identify the most informative ones. We'll use the feature_importances_ attribute of the Random Forest Regressor to rank the components based on their importance.\n",
        "\n",
        "Here’s how we can proceed:\n",
        "\n",
        "**Step-by-step approach:**\n",
        "1. Train the model using the best hyperparameters (already done).\n",
        "2. Extract the feature importances from the model.\n",
        "3. Visualize the importance of each PCA component using a bar plot.\n",
        "4. Analyze the results and potentially drop the least important components."
      ],
      "metadata": {
        "id": "_TthRd9M8sOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refit the model with the best hyperparameters and 8 PCA components (or the number of components kept after PCA)\n",
        "rf_regressor = RandomForestRegressor(n_estimators=300, min_samples_split=2, min_samples_leaf=1,\n",
        "                                     max_depth=None, bootstrap=True, random_state=42)\n",
        "\n",
        "# Fit the Random Forest Regressor to the training data\n",
        "rf_regressor.fit(X_train[:, :num_components_to_keep], y_train)  # Use only the number of components we retained\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Sort the feature importances in descending order and get their corresponding indices\n",
        "indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Plot the feature importances for the retained PCA components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance of PCA Components\")\n",
        "plt.bar(range(num_components_to_keep), feature_importances[indices], align=\"center\")\n",
        "plt.xticks(range(num_components_to_keep), [f'PC{i+1}' for i in indices], rotation=45)\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "obFpQNey9Asg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Feature Importance: Why PC8 and PC6 Are Critical\n",
        "\n",
        "In the context of our analysis, **PC8** and **PC6** emerged as the most influential principal components in the Random Forest model. Their high importance scores indicate that they capture the most variance in the data that correlates with the target variable, `xlogp`, which represents the partition coefficient of the compounds.\n",
        "\n",
        "1. **Why PC8 and PC6 Are Important:**\n",
        "   - PCA works by transforming the original features into a set of orthogonal components (PCs) that represent the directions of maximum variance in the data. The higher the importance of a component, the more strongly it explains the variance associated with the target variable.\n",
        "   - **PC8** and **PC6** likely capture critical information related to the structural features and chemical properties of the compounds that most affect their partition coefficient.\n",
        "   - These components may represent hidden interactions between variables like molecular weight, hydrogen bonding, or molecular complexity that are not obvious from the individual original features.\n",
        "\n",
        "2. **How We Will Use This Data:**\n",
        "   - **Model Optimization**: We will focus on the most important components (such as PC8 and PC6) to streamline our model, potentially reducing the number of features while maintaining or improving prediction accuracy.\n",
        "   - **Feature Selection**: The feature importance rankings help us decide which components to prioritize in future iterations of the model. This allows us to ignore less influential components like PC4 or PC2, simplifying the model without significant loss in performance.\n",
        "   - **Biological Interpretation**: Understanding why certain PCs are important could guide us in interpreting the chemical or structural properties that most affect the antibiotic properties of the molecules. This insight can inform the design of new antibiotics by focusing on the most critical molecular features captured by PC8 and PC6.\n",
        "\n",
        "In conclusion, the importance of PC8 and PC6 in the model highlights key latent features in the data that we can leverage for more targeted predictions and to refine our feature selection process in future analyses.\n"
      ],
      "metadata": {
        "id": "6DQMt0qF-F7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by delving deeper into PC8 and PC6 to understand which original features (chemical properties) contribute most to their importance. The PCA loadings represent how much each feature contributes to each principal component. We'll generate a visualization or table showing the loadings for these components.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Extract PCA Loadings: We'll extract the loading values for each feature in PC8 and PC6.\n",
        "2. Visualize the Loadings: We'll create a bar chart to visually represent the loadings of each feature in these components. The higher the absolute value of the loading, the more that feature contributes to the principal component.\n",
        "3. Interpretation: Once we have the loadings, we can analyze the most significant features in PC8 and PC6, which are driving their importance.\n",
        "\n",
        "Let’s first extract and display the loadings for PC8 and PC6.\n",
        "\n",
        "PCA Loadings for PC8 and PC6:\n",
        "We’ll start by generating a table and visualizing the loadings for these components to understand the contribution of each feature."
      ],
      "metadata": {
        "id": "Lk4slluI-v7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the number of columns match the features used in PCA\n",
        "numerical_columns_for_pca = numerical_columns[['mw', 'polararea', 'complexity', 'xlogp', 'heavycnt',\n",
        "                                               'hbonddonor', 'hbondacc', 'rotbonds', 'exactmass',\n",
        "                                               'monoisotopicmass', 'charge', 'covalentunitcnt',\n",
        "                                               'isotopeatomcnt', 'totalatomstereocnt', 'definedatomstereocnt',\n",
        "                                               'undefinedatomstereocnt', 'totalbondstereocnt',\n",
        "                                               'definedbondstereocnt', 'undefinedbondstereocnt', 'pclidcnt',\n",
        "                                               'gpidcnt']]\n",
        "\n",
        "# Extract the PCA components into a DataFrame\n",
        "pca_components_df = pd.DataFrame(pca.components_, columns=numerical_columns_for_pca.columns)\n",
        "\n",
        "# Extract loadings for PC8 and PC6\n",
        "pc8_loadings = pca_components_df.iloc[7]  # PC8 is the 8th component (index 7)\n",
        "pc6_loadings = pca_components_df.iloc[5]  # PC6 is the 6th component (index 5)\n",
        "\n",
        "# Create a bar chart for PC8 loadings\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(pc8_loadings.index, pc8_loadings.values)\n",
        "plt.title('Feature Loadings for PC8')\n",
        "plt.xlabel('Loading Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Create a bar chart for PC6 loadings\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(pc6_loadings.index, pc6_loadings.values)\n",
        "plt.title('Feature Loadings for PC6')\n",
        "plt.xlabel('Loading Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "# Display loadings as tables for further inspection\n",
        "print(\"Loadings for PC8:\")\n",
        "print(pc8_loadings.sort_values(ascending=False))\n",
        "\n",
        "print(\"\\nLoadings for PC6:\")\n",
        "print(pc6_loadings.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "_dui8Oua-_Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Principal Components PC8 and PC6:\n",
        "\n",
        "From the results and visualization, we observe that **PC8** and **PC6** play crucial roles, with specific features contributing significantly to each.\n",
        "\n",
        "#### Analysis of PC8:\n",
        "- **Key Contributors**: `gpidcnt`, `covalentunitcnt`, and `heavycnt` have the highest positive contributions, while `definedbondstereocnt` shows the largest negative contribution.\n",
        "- **Chemical Insights**:\n",
        "  - **gpidcnt (Gene Product ID Count)** seems to dominate the variance in PC8. This feature is tied to biological properties and can be important for identifying relationships in molecular design.\n",
        "  - **Covalent Unit Count** and **Heavy Atom Count** are important for structural stability and complexity in molecules.\n",
        "  - **Defined Bond Stereocenters** show a negative loading, which may indicate a need to minimize certain stereochemistry constraints in future antibiotic designs to optimize the effectiveness of the compounds.\n",
        "\n",
        "#### Analysis of PC6:\n",
        "- **Key Contributors**: `totalbondstereocnt` and `isotopeatomcnt` have the highest positive contributions, while `totalatomstereocnt` and `undefinedatomstereocnt` contribute negatively.\n",
        "- **Chemical Insights**:\n",
        "  - **Total Bond Stereocenter Count** plays a strong role in PC6, showing the importance of stereochemistry and bond orientation in the chemical structure.\n",
        "  - **Isotope Atom Count** also contributes, which might suggest an importance in isotopic labeling or diversity in the design of new antibiotics.\n",
        "  - **Total Atom Stereocenter Count** has a negative loading, indicating that we might want to reduce the stereocenters to simplify molecule design without losing efficacy.\n",
        "\n",
        "### Next Steps:\n",
        "We can use this information to design new antibiotics that focus on optimizing or minimizing these chemical properties (i.e., increase `gpidcnt`, simplify stereochemistry) based on their influence on the principal components. These insights can be fed into the **Graph Neural Network (GNN)** for molecular generation, leveraging the importance of specific chemical properties identified through PCA.\n",
        "\n",
        "We will proceed with this feature analysis in the GNN model to generate potential new antibiotic structures, streamlining based on the insights gathered from **PC8** and **PC6**.\n"
      ],
      "metadata": {
        "id": "apFzK2db_0WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview: Building and Training a Graph Neural Network (GNN) for Antibiotic Discovery\n",
        "\n",
        "### Objective\n",
        "The primary goal of this project is to leverage **Graph Neural Networks (GNNs)** to predict molecular properties related to antibiotics. Specifically, we aim to analyze the molecular structure of compounds and use their graph representation to forecast critical properties, such as the partition coefficient (**xlogp**), which can be related to the drug's behavior in biological systems.\n",
        "\n",
        "This work is part of the larger effort to discover new antibiotics using machine learning, leveraging the power of GNNs to capture the intricate relationships between atoms and bonds in molecular structures.\n",
        "\n",
        "### Why Use GNNs?\n",
        "Unlike traditional machine learning models, **GNNs** are particularly well-suited to handle graph-structured data. In chemistry, molecules can naturally be represented as graphs:\n",
        "- **Atoms** are the **nodes**.\n",
        "- **Bonds** between atoms are the **edges**.\n",
        "This graph representation allows GNNs to learn meaningful patterns from molecular structures, which are critical in predicting properties like solubility, reactivity, and drug efficacy.\n",
        "\n",
        "### Why are we interested in xlogp?\n",
        "The **xlogp** (partition coefficient) is a key molecular descriptor that indicates how a compound distributes between aqueous (water) and lipid (fat) environments. In drug discovery, understanding a molecule's xlogp value helps in assessing its bioavailability, permeability, and how it behaves in biological systems. Hence, accurately predicting xlogp using GNNs may accelerate the identification of viable antibiotic candidates.\n",
        "\n",
        "![xlogp](https://cheminfographic.wordpress.com/wp-content/uploads/2020/05/partition-coefficient-logp.jpg?w=1194)\n",
        "\n",
        "\n",
        "### Steps Taken\n",
        "\n",
        "1. **Graph Representation of Molecules**:\n",
        "   Each molecule is represented as a graph, where:\n",
        "   - **Nodes** represent atoms.\n",
        "   - **Edges** represent bonds between atoms.\n",
        "\n",
        "   For each molecule, we create:\n",
        "   - An **adjacency matrix** to indicate the bonds.\n",
        "   - **Node features** representing atomic properties (e.g., molecular weight, polarity, etc.).\n",
        "\n",
        "2. **Feature Selection and PCA**:\n",
        "   Earlier in our analysis, we identified the most informative features using **Principal Component Analysis (PCA)**, which reduced the complexity of our feature set while retaining most of the variance in the data. These features are used as node properties in the GNN.\n",
        "\n",
        "3. **Model Setup**:\n",
        "   We have chosen a **Graph Convolutional Network (GCN)**, a type of GNN, as the architecture for our model. This model takes the molecular graph as input and predicts the xlogp value:\n",
        "   - **Graph Convolution Layers**: These layers operate over graph-structured data to propagate information between atoms (nodes).\n",
        "   - **Pooling**: We apply a **global pooling** layer that aggregates the node features into a single representation for each molecule.\n",
        "   - **Output Layer**: A final dense layer for regression predicts the xlogp value.\n",
        "\n",
        "4. **Training the Model**:\n",
        "   We train the model using known molecular data and xlogp values to allow the GNN to learn how to predict the partition coefficient from molecular structures.\n",
        "\n",
        "### Why This Approach?\n",
        "The molecular structure plays a crucial role in a drug's effectiveness. By employing GNNs, we leverage the graph-like nature of molecules to make more accurate predictions based on the relationships between atoms. Unlike traditional approaches that treat molecules as flat vectors, GNNs can take advantage of the full structure of the molecule, enabling us to discover new insights and potentially uncover novel antibiotics.\n",
        "\n",
        "### Expected Outcome\n",
        "We expect that the GNN model will accurately predict the xlogp values for the molecules in our dataset. By identifying key properties and molecular patterns associated with successful antibiotics, we aim to accelerate the drug discovery process. In the long run, this approach can help to uncover new antibiotic compounds that could combat resistant bacteria and other pathogens.\n",
        "\n"
      ],
      "metadata": {
        "id": "6cIyEh1KBiGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Graph Representation Preparation\n",
        "We'll represent each molecule as a graph where:\n",
        "\n",
        "* Nodes represent atoms.\n",
        "* Edges represent bonds (using an adjacency matrix).\n",
        "For each molecule, we need two things:\n",
        "\n",
        "An Adjacency Matrix that represents which atoms are connected (i.e., which bonds exist).\n",
        "Node Features, which are the properties of atoms (such as molecular weight, xlogp, etc.).\n",
        "\n",
        "We'll need Spektral to handle the GNN part.\n"
      ],
      "metadata": {
        "id": "eWpyHv-fB67y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral\n"
      ],
      "metadata": {
        "id": "R3-mD41i_NWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Graph Data**\n",
        "\n",
        "In order to do this we must first:\n",
        "\n",
        "## Extract Adjacency Matrix and Node Features:\n",
        "\n",
        "In order to accurately represent molecular structures as graphs, we need to first extract the **adjacency matrices** and **node features** (atomic properties) from the actual molecular data.\n",
        "\n",
        "Here's how we can approach this:\n",
        "\n",
        "### Adjacency Matrix:\n",
        "The adjacency matrix of a molecule represents the bonds between atoms. In this matrix:\n",
        "- Rows and columns correspond to atoms in the molecule.\n",
        "- If there is a bond between atom $i$ and atom $j$, then the matrix at position $(i,j)$ will have a value of 1.\n",
        "- If there is no bond, the value is 0.\n",
        "\n",
        "### Node Features:\n",
        "Each node (atom) can have multiple features like atomic number, degree, charge, or molecular weight. These features can be extracted from the molecular structure and used as input into the GNN.\n",
        "\n",
        "### Steps to Extract Adjacency Matrix and Node Features:\n",
        "\n",
        "#### Step 1: Extract Molecular Data\n",
        "If you're using a file or a dataset that contains chemical structures, you can use a chemistry toolkit like **RDKit** to convert SMILES (Simplified Molecular Input Line Entry System) strings into molecular graphs.\n",
        "\n",
        "#### Step 2: Generate the Adjacency Matrix\n",
        "Using **RDKit** or a similar tool, you can generate the adjacency matrix for each molecule based on its bonds.\n",
        "\n",
        "#### Step 3: Extract Node Features\n",
        "The atomic properties of each atom can be extracted from the molecular structure, providing node features for the GNN.\n"
      ],
      "metadata": {
        "id": "Z3YdHcEFCW3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi\n"
      ],
      "metadata": {
        "id": "WsdApNbrEi10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdmolops\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert a molecule (SMILES) to a graph with adjacency matrix and node features\n",
        "def molecule_to_graph(smiles):\n",
        "    # Convert SMILES string to an RDKit molecule object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # If the SMILES string is invalid, raise an error\n",
        "    if mol is None:\n",
        "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
        "\n",
        "    # Get adjacency matrix from the molecule (bonds between atoms)\n",
        "    adjacency_matrix = rdmolops.GetAdjacencyMatrix(mol)\n",
        "\n",
        "    # Extract node features (for now, using atomic number, but can be expanded)\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_features = [atom.GetAtomicNum()]  # Atomic number is used as a node feature\n",
        "        node_features.append(atom_features)\n",
        "\n",
        "    # Convert list of node features to a numpy array\n",
        "    node_features = np.array(node_features)\n",
        "\n",
        "    return adjacency_matrix, node_features\n",
        "\n",
        "# Load the dataset that contains SMILES strings (adjust the path to match your dataset location)\n",
        "# Assuming the dataset has a column named 'isosmiles' containing the SMILES strings\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/PubChem_compound_text_Antibiotics.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Placeholder to store graphs (adjacency matrices and node features)\n",
        "graph_data = []\n",
        "\n",
        "# Iterate over the dataset and process each molecule's SMILES string\n",
        "for idx, row in df.iterrows():\n",
        "    smiles = row['isosmiles']  # Change 'isosmiles' to the actual column name for SMILES strings\n",
        "    try:\n",
        "        adj_matrix, node_features = molecule_to_graph(smiles)\n",
        "        graph_data.append({\n",
        "            'smiles': smiles,\n",
        "            'adjacency_matrix': adj_matrix,\n",
        "            'node_features': node_features\n",
        "        })\n",
        "        print(f\"Processed molecule {idx + 1}/{len(df)}\")\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "# Example of how to access the adjacency matrix and node features for the first molecule\n",
        "first_graph = graph_data[0]\n",
        "print(\"First Molecule SMILES:\", first_graph['smiles'])\n",
        "print(\"First Molecule Adjacency Matrix:\\n\", first_graph['adjacency_matrix'])\n",
        "print(\"First Molecule Node Features:\\n\", first_graph['node_features'])\n"
      ],
      "metadata": {
        "id": "JR7nntylEXwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Graph Dataset for GNN Input\n",
        "\n",
        "Now that we have extracted the molecular data, including adjacency matrices and node features, we need to organize this data into a format that is suitable for training a Graph Neural Network (GNN). We will use the TensorFlow GNN library to create a graph dataset.\n",
        "\n",
        "#### Key Steps:\n",
        "\n",
        "1. **Graph Representation:**\n",
        "   - Each molecule is represented as a graph where:\n",
        "     - Nodes represent atoms.\n",
        "     - Edges represent bonds between atoms.\n",
        "   - The adjacency matrix represents the connectivity of the graph.\n",
        "   - Node features contain atomic properties such as atomic number, charge, or molecular weight.\n",
        "\n",
        "2. **TensorFlow GNN GraphTensor:**\n",
        "   - TensorFlow GNN uses `GraphTensor` as the primary data structure to hold graphs.\n",
        "   - Each `GraphTensor` consists of:\n",
        "     - `node_sets`: A set of nodes (atoms) with their features.\n",
        "     - `edge_sets`: A set of edges (bonds) defined by the adjacency matrix.\n",
        "   \n",
        "3. **Batching Graphs:**\n",
        "   - We will batch multiple molecular graphs together for efficient training in the GNN.\n",
        "\n",
        "#### Objective:\n",
        "We will convert each molecule's adjacency matrix and node features into a `GraphTensor` format that can be fed into a GNN. This will enable the model to learn molecular representations and properties.\n",
        "\n",
        "## **Prepare the dataset for the GNN using TensorFlow GNN:**"
      ],
      "metadata": {
        "id": "otaIAKTpYJCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "\n"
      ],
      "metadata": {
        "id": "OnKFKeU-ZHt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We Are Switching to PyTorch Geometric for GNN Implementation (\"Everything Fails All The Time\")\n",
        "\n",
        "❌ In the process of developing code, one has to be ready to deal with incompatibility issues. Here we encounter just such a scenario. But things are still good to go 👌\n",
        "\n",
        "Initially, we started using **TensorFlow GNN** for building our Graph Neural Network (GNN) model. However, we encountered a version compatibility issue due to the Keras version that TensorFlow GNN requires. Specifically, **TensorFlow GNN requires Keras version 2**, while our current environment uses **Keras version 3**, which leads to incompatibility.\n",
        "\n",
        "Instead of modifying the environment by reverting to an older version of Keras, we have decided to switch to **PyTorch Geometric (PyG)**. Here are the reasons for this switch:\n",
        "\n",
        "#### 1. **PyTorch Geometric Is Purpose-Built for Graph Neural Networks**\n",
        "   - PyTorch Geometric is designed specifically for graph data, providing optimized operations and utilities for handling graph structures such as adjacency matrices, edge lists, and node features.\n",
        "   - PyG has a large set of pre-built layers and functions that can efficiently handle various GNN architectures, including **GCN, GAT, and GraphSAGE**.\n",
        "\n",
        "#### 2. **Version Compatibility**\n",
        "   - Unlike TensorFlow GNN, **PyTorch Geometric works seamlessly with the latest version of PyTorch**, allowing us to avoid compatibility issues.\n",
        "   - PyG is regularly updated and works well with modern deep learning environments, making it more flexible for our use case.\n",
        "\n",
        "#### 3. **Ease of Use with Molecular Graph Data**\n",
        "   - PyTorch Geometric provides efficient ways to handle molecular data, including **adjacency matrices and node features**, which can be easily converted into PyG’s `Data` format.\n",
        "   - PyG simplifies the creation, training, and evaluation of GNN models, especially for tasks such as molecular property prediction, which is crucial in our project of designing new antibiotics.\n",
        "\n",
        "#### 4. **Strong Community Support and Documentation**\n",
        "   - PyTorch Geometric has strong community support, and its documentation provides detailed guides and examples, making it easier for us to develop and troubleshoot our GNN models.\n",
        "   - There are also plenty of tutorials and open-source resources available for GNN model development using PyG.\n",
        "\n",
        "#### 5. **Compatibility with Future Tasks**\n",
        "   - PyTorch Geometric integrates well with other PyTorch functionalities, making it a good choice for more advanced tasks that we may encounter later, such as **transfer learning**, **self-supervised learning**, or more complex GNN architectures.\n",
        "\n",
        "### TL;DR:\n",
        "By switching to PyTorch Geometric, we ensure that we have a flexible, optimized, and compatible framework for building and training our GNN models. This will allow us to proceed with the molecular graph data analysis without dealing with versioning issues or other complications.\n"
      ],
      "metadata": {
        "id": "dMledCzeamQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛑 🛠 Additional Problem: we must adapt the logic for PyTorch Geometric to make it compatible with our dataset of molecular graphs.\n",
        "\n",
        "**Here's how we can adjust it:**\n",
        "1. Convert adjacency matrices and node features to PyTorch Geometric's Data format.\n",
        "2. Batch the graphs and use PyTorch's data loaders to handle graph datasets efficiently.\n",
        "3. Implement the GNN model using PyG's pre-built layers like GCNConv, GraphConv, or other relevant layers for graph-based tasks."
      ],
      "metadata": {
        "id": "pg-kTRijdZiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter\n"
      ],
      "metadata": {
        "id": "o5SyeGPJY8d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-sparse\n"
      ],
      "metadata": {
        "id": "QKOB5LFReJz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "V641M-oBq_Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "id": "LDs92Iyg17Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Adjacency Matrices and Node Features"
      ],
      "metadata": {
        "id": "GNnUWfw3-Tqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "# Assuming your dataframe is named 'df' and the SMILES strings are in the 'isosmiles' column\n",
        "smiles_list = df['isosmiles'].dropna().tolist()  # Drop any NaN values\n",
        "\n",
        "# Initialize empty lists to hold adjacency matrices and node features\n",
        "adjacency_matrices = []\n",
        "node_features_list = []\n",
        "\n",
        "# Function to convert SMILES to adjacency matrix and node features\n",
        "def molecule_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None, None  # Handle invalid SMILES\n",
        "\n",
        "    # Get adjacency matrix\n",
        "    adj_matrix = Chem.GetAdjacencyMatrix(mol)\n",
        "\n",
        "    # Extract node features (for simplicity, using atomic numbers here)\n",
        "    node_features = [[atom.GetAtomicNum()] for atom in mol.GetAtoms()]\n",
        "\n",
        "    return adj_matrix, node_features\n",
        "\n",
        "# Process the SMILES strings from the dataframe\n",
        "for i, smiles in enumerate(smiles_list):\n",
        "    adj_matrix, node_features = molecule_to_graph(smiles)\n",
        "    if adj_matrix is not None and node_features is not None:\n",
        "        adjacency_matrices.append(adj_matrix)\n",
        "        node_features_list.append(node_features)\n",
        "    print(f\"Processed molecule {i+1}/{len(smiles_list)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "b8RqvbW--cFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Function to convert molecule data to PyTorch Geometric Data format\n",
        "def create_graph_data(adj_matrix, node_features):\n",
        "    # Convert adjacency matrix and node features to PyTorch tensors\n",
        "    edge_index = torch.tensor(list(zip(*torch.where(torch.tensor(adj_matrix) == 1))), dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Create the PyTorch Geometric Data object\n",
        "    graph_data = Data(x=x, edge_index=edge_index)\n",
        "    return graph_data\n",
        "\n",
        "# Create a list of Data objects for the entire dataset\n",
        "graph_data_list = []\n",
        "for i in range(len(adjacency_matrices)):\n",
        "    graph_data = create_graph_data(adjacency_matrices[i], node_features_list[i])\n",
        "    graph_data_list.append(graph_data)\n",
        "\n",
        "# Batch the graphs for training\n",
        "loader = DataLoader(graph_data_list, batch_size=32, shuffle=True)\n",
        "\n",
        "# Print a sample batched graph to verify the format\n",
        "for batch in loader:\n",
        "    print(batch)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "XFffr0Rf-5kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output DataBatch(x=[1554, 1], edge_index=[2, 3358], batch=[1554], ptr=[33]) indicates that the molecular graph data has been successfully batched for GNN training.\n",
        "\n",
        "Here’s what the dimensions represent:\n",
        "\n",
        "* x=[1554, 1]: This means the total number of nodes across all graphs in the batch is 1,554, and each node has 1 feature (e.g., an atomic feature).\n",
        "* edge_index=[2, 3358]: This represents the edges between the nodes, where 3,358 pairs of nodes have connections (bonds), and edge_index provides the source and target nodes for each bond.\n",
        "* batch=[1554]: This tensor assigns each of the 1,554 nodes to one of the 32 graphs in the batch. It helps the model understand which nodes belong to which graph.\n",
        "* ptr=[33]: This points to the cumulative sum of the number of nodes in each graph, used for efficient batching.\n",
        "\n",
        "## GNN Model for Classification"
      ],
      "metadata": {
        "id": "hUMNqHxG_kDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Define a simple GCN model for binary classification\n",
        "class GCNBinaryClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GCNBinaryClassifier, self).__init__()\n",
        "        # First Graph Convolutional Layer\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        # Second Graph Convolutional Layer\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        # Output Layer (for binary classification)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First Graph Convolution + ReLU activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution + ReLU activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Global pooling and final linear layer\n",
        "        x = torch.mean(x, dim=0)  # Global mean pooling (for graph-level predictions)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return torch.sigmoid(x)  # Sigmoid activation for binary classification\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1  # Number of node features (e.g., atomic features)\n",
        "hidden_dim = 64  # Adjust based on model complexity\n",
        "model = GCNBinaryClassifier(input_dim, hidden_dim)\n"
      ],
      "metadata": {
        "id": "m-3qQyH8_RBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting the Training Loop for Classification:\n",
        "Since it's a classification task, we will use binary cross-entropy loss instead of MSE.\n",
        "\n",
        "### Define DataLoader for Training and Validation:\n"
      ],
      "metadata": {
        "id": "6noOZJMSA-F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Assuming we have graph_data_list created\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.8 * len(graph_data_list))\n",
        "val_size = len(graph_data_list) - train_size\n",
        "\n",
        "train_data = graph_data_list[:train_size]\n",
        "val_data = graph_data_list[train_size:]\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "8kVh-zcABFTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Target Labels to Data Object"
      ],
      "metadata": {
        "id": "56ZNFBSwBx-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the function to include target (y) in Data object\n",
        "def create_graph_data(adj_matrix, node_features, target_label):\n",
        "    # Convert adjacency matrix and node features to PyTorch tensors\n",
        "    edge_index = torch.tensor(list(zip(*torch.where(torch.tensor(adj_matrix) == 1))), dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Create the PyTorch Geometric Data object\n",
        "    graph_data = Data(x=x, edge_index=edge_index, y=torch.tensor([target_label], dtype=torch.float))\n",
        "    return graph_data\n"
      ],
      "metadata": {
        "id": "Y9Jg4gznBg7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensure Target Labels are Passed When Creating Graph Data"
      ],
      "metadata": {
        "id": "xDNJ0018B9Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DV04Kvo-B41F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}